<!DOCTYPE html>
<html>
  <head>
    <title>Statistical Thinking using Randomisation and Simulation</title>
    <meta charset="utf-8">
    <meta name="author" content="Di Cook University" />
    <link href="week6.class2_files/remark-css-0.0.1/example.css" rel="stylesheet" />
    <link rel="stylesheet" href="myremark.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Statistical Thinking using Randomisation and Simulation
## Generalised Linear Models
### Di Cook University
### W6.C2

---




# Generalised linear models

- Overview
- Types
- Assumptions
- Fitting
- Examples


---
# Overview

- GLMs are a broad class of models for fitting different types of response variables distributions. 
- The multiple linear regression model is a special case.

---
# Three components

- Random Component: probability distribution of the response variable
- Systematic Component: explanatory variables
- Link function: describes the relaionship between the random and systematic components

---
# Multiple linear regression

`$$y_i = \beta_0+\beta_1x_1 + \beta_2x_2 + \varepsilon ~~~ or ~~~ E(Y_i)=\beta_0+\beta_1x_1+\beta_2x_2$$`

- Random component: `\(y_i\)` has a normal distribution, and so `\(e_i \sim N(0,\sigma^2)\)`
- Systematic component: `\(\beta_0+\beta_1x_1 + \beta_2x_2\)`
- Link function: identity, just the systematic component

---
# Poisson regression

`$$y_i = exp(\beta_0+\beta_1x_1+\beta_2x_2) + \varepsilon$$`

- `\(y_i\)` takes integer values, 0, 1, 2, ...
- Link function: `\(ln(\mu)\)`, name=`log`. (Think of `\(\mu\)` as `\(\hat{y}\)`.)

---
# Bernouilli, binomial regression

`$$y_i = \frac{exp(\beta_0+\beta_1x_1+\beta_2x_2)}{1+exp(\beta_0+\beta_1x_1+\beta_2x_2)} + \varepsilon$$`

- `\(y_i\)` takes integer values, `\(\{ 0, 1\}\)` (bernouilli), `\(\{ 0, 1, ..., n\}\)` (binomial)
- Let `\(\mu=\frac{exp(\beta_0+\beta_1x_1+\beta_2x_2)}{1+exp(\beta_0+\beta_1x_1+\beta_2x_2)}\)`, link function is `\(ln\frac{\mu}{1-\mu}\)`, name=`logit`

---
# Assumptions

- The data `\(y_1, y_2, ..., y_n\)` are independently distributed, i.e., cases are independent.
- The dependent variable `\(y_i\)` does NOT need to be normally distributed, but it typically assumes a distribution from an exponential family (e.g. binomial, Poisson, multinomial, normal,...)
- Linear relationship between the transformed response (see examples below)
- Explanatory variables can be transformations of original variables
- Homogeneity of variance does NOT need to be satisfied for original units, but it should be still true on the transformed response scale
- Uses maximum likelihood estimation (MLE) to estimate the parameters
- Goodness-of-fit measures rely on sufficiently large samples

---
# Example: Olympics medal tally

- Model medal counts on log_GDP
- Medal counts = integer, which suggests using a Poisson model. 

&lt;img src="week6.class2_files/figure-html/unnamed-chunk-1-1.png" style="display: block; margin: auto;" /&gt;

---
# Model fit and what it looks like



```r
oly_glm &lt;- glm(M2012~GDP_log, data=oly_gdp2012,
               family=poisson(link=log))
summary(oly_glm)$coefficients
#&gt;             Estimate Std. Error z value Pr(&gt;|z|)
#&gt; (Intercept)    -13.2      0.538     -24 3.6e-132
#&gt; GDP_log          1.3      0.045      30 6.8e-198
```


&lt;img src="week6.class2_files/figure-html/unnamed-chunk-3-1.png" style="display: block; margin: auto;" /&gt;

---
class: inverse middle 
# Your turn

Write down the formula of the fitted model.

--
`$$\hat{log(M2012)} = -13.2 +1.3 GDP.log$$`

---
# Model fit


```
#&gt; 
#&gt; Call:
#&gt; glm(formula = M2012 ~ GDP_log, family = poisson(link = log), 
#&gt;     data = oly_gdp2012)
#&gt; 
#&gt; Deviance Residuals: 
#&gt;    Min      1Q  Median      3Q     Max  
#&gt;  -4.80   -2.22   -0.36    1.07    8.55  
#&gt; 
#&gt; Coefficients:
#&gt;             Estimate Std. Error z value Pr(&gt;|z|)    
#&gt; (Intercept) -13.1691     0.5383   -24.5   &lt;2e-16 ***
#&gt; GDP_log       1.3406     0.0447    30.0   &lt;2e-16 ***
#&gt; ---
#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
#&gt; 
#&gt; (Dispersion parameter for poisson family taken to be 1)
#&gt; 
#&gt;     Null deviance: 1567.70  on 84  degrees of freedom
#&gt; Residual deviance:  545.92  on 83  degrees of freedom
#&gt; AIC: 845.7
#&gt; 
#&gt; Number of Fisher Scoring iterations: 5
```

The difference between the null and residual deviance is substantial, suggesting a good fit.  

---
# Residual plots

![](week6.class2_files/figure-html/unnamed-chunk-5-1.png)&lt;!-- --&gt;

Heteroskedasticity in residuals. One fairly large residual.

---
# Influence statistics


```
#&gt;         .rownames .cooksd .resid
#&gt; 1      RussianFed 1.9e+00  8.553
#&gt; 2           China 1.5e+00  3.743
#&gt; 3    UnitedStates 8.3e-01  1.468
#&gt; 4    GreatBritain 8.0e-01  5.232
#&gt; 5         Jamaica 4.4e-01  5.267
#&gt; 6           India 2.6e-01 -4.800
#&gt; 7           Japan 2.5e-01 -2.010
#&gt; 8            Cuba 2.4e-01  4.215
#&gt; 9         Ukraine 2.3e-01  4.270
#&gt; 10          Kenya 1.9e-01  3.802
#&gt; 11        Belarus 1.6e-01  3.535
#&gt; 12        Hungary 1.5e-01  3.621
#&gt; 13         Brazil 1.5e-01 -2.862
#&gt; 14        Georgia 1.3e-01  3.219
#&gt; 15      Indonesia 1.2e-01 -4.563
#&gt; 16         Mexico 9.8e-02 -3.444
#&gt; 17    SaudiArabia 9.2e-02 -4.388
#&gt; 18      Australia 7.6e-02  2.211
#&gt; 19     Azerbaijan 7.5e-02  2.584
#&gt; 20       Mongolia 7.3e-02  2.612
#&gt; 21  ChineseTaipei 7.0e-02 -3.680
#&gt; 22         Turkey 6.5e-02 -3.179
#&gt; 23    Switzerland 6.5e-02 -3.293
#&gt; 24       Ethiopia 6.2e-02  2.385
#&gt; 25        Belgium 6.0e-02 -3.294
#&gt; 26      Venezuela 5.8e-02 -3.498
#&gt; 27     NewZealand 5.0e-02  2.211
#&gt; 28  HongKongChina 4.9e-02 -3.191
#&gt; 29       Portugal 4.9e-02 -3.164
#&gt; 30         Greece 4.5e-02 -2.932
#&gt; 31     Kazakhstan 4.4e-02  2.100
#&gt; 32         Norway 4.3e-02 -2.700
#&gt; 33       DPRKorea 4.2e-02  2.020
#&gt; 34        Algeria 4.0e-02 -2.815
#&gt; 35      Singapore 3.9e-02 -2.705
#&gt; 36      Argentina 3.8e-02 -2.534
#&gt; 37         Kuwait 3.8e-02 -2.731
#&gt; 38       Thailand 3.7e-02 -2.566
#&gt; 39       Malaysia 3.7e-02 -2.602
#&gt; 40         Canada 3.6e-02 -1.607
#&gt; 41          Egypt 3.4e-02 -2.512
#&gt; 42          Korea 3.3e-02  1.635
#&gt; 43        Finland 2.9e-02 -2.222
#&gt; 44          Spain 2.6e-02 -1.463
#&gt; 45          Qatar 2.6e-02 -2.126
#&gt; 46        Morocco 2.4e-02 -2.147
#&gt; 47        Germany 2.1e-02  0.754
#&gt; 48    SouthAfrica 1.9e-02 -1.705
#&gt; 49         Sweden 1.8e-02 -1.586
#&gt; 50        Armenia 1.4e-02  1.291
#&gt; 51 TrinidadTobago 1.4e-02  1.234
#&gt; 52     PuertoRico 1.2e-02 -1.390
#&gt; 53      Guatemala 1.1e-02 -1.396
#&gt; 54        Croatia 1.1e-02  1.073
#&gt; 55      Lithuania 1.0e-02  1.072
#&gt; 56        Ireland 7.5e-03 -1.044
#&gt; 57  CzechRepublic 5.5e-03  0.804
#&gt; 58        Grenada 5.2e-03  1.025
#&gt; 59    Netherlands 5.2e-03  0.726
#&gt; 60         Poland 5.0e-03 -0.817
#&gt; 61  Rep.ofMoldova 4.9e-03  0.827
#&gt; 62        Romania 4.8e-03  0.750
#&gt; 63        Bahrain 4.8e-03 -0.925
#&gt; 64         Cyprus 4.6e-03 -0.904
#&gt; 65  DominicanRep. 4.4e-03 -0.822
#&gt; 66       Bulgaria 4.4e-03 -0.820
#&gt; 67     Uzbekistan 2.6e-03  0.555
#&gt; 68         Serbia 2.5e-03  0.550
#&gt; 69    Afghanistan 2.3e-03 -0.637
#&gt; 70       Colombia 2.0e-03 -0.518
#&gt; 71          Gabon 1.9e-03 -0.588
#&gt; 72       Botswana 1.9e-03 -0.575
#&gt; 73          Italy 1.8e-03 -0.304
#&gt; 74         Uganda 1.7e-03 -0.558
#&gt; 75       Slovenia 1.1e-03  0.359
#&gt; 76       Slovakia 9.6e-04 -0.360
#&gt; 77        Denmark 8.2e-04 -0.330
#&gt; 78     Montenegro 3.6e-04  0.257
#&gt; 79         Latvia 2.4e-04 -0.189
#&gt; 80        Tunisia 8.8e-05 -0.109
#&gt; 81        Bahamas 7.5e-05 -0.116
#&gt; 82         France 4.7e-05  0.042
#&gt; 83           Iran 3.5e-06 -0.021
#&gt; 84        Estonia 3.5e-06 -0.022
#&gt; 85     Tajikistan 8.3e-07 -0.012
```

Largest Cooks D values enough to have some concerns about the influence that Russian Federation and China have on the model fit. Should re-fit without these two cases.

---
# Prediction from the model


```r
aus &lt;- oly_gdp2012 %&gt;% filter(Code == "AUS")
predict(oly_glm, aus)
#&gt;   1 
#&gt; 3.2
```

WAIT! What??? Australia earned more than 3 medals in 2012. Either the model is terrible, or we've made a mistake!

--

```r
aus &lt;- oly_gdp2012 %&gt;% filter(Code == "AUS")
predict(oly_glm, aus, type="response")
#&gt;  1 
#&gt; 23
```

--
Need to transform predictions into original units.

---
# Example: winning tennis matches

We have data scraped from the web sites of the 2012 Grand Slam tennis tournaments. There are a lot of statistics on matches. Below we have the number of receiving points won, and whether the match was won or not. 

&lt;img src="week6.class2_files/figure-html/unnamed-chunk-9-1.png" style="display: block; margin: auto;" /&gt;

---
class: inverse middle 
# Your turn

The response variable is binary. What type of GLM should be fit?  

--
*bernouilli/binomial*

---
# Model


```r
tennis_glm &lt;- glm(won~Receiving.Points.Won, data=tennis,
                  family=binomial(link='logit'))
```


```
#&gt;                      Estimate Std. Error z value Pr(&gt;|z|)
#&gt; (Intercept)             -2.91      0.586    -5.0  7.1e-07
#&gt; Receiving.Points.Won     0.11      0.015     7.3  3.0e-13
```


&lt;img src="week6.class2_files/figure-html/unnamed-chunk-12-1.png" style="display: block; margin: auto;" /&gt;

---
class: inverse middle 
# Your turn

Write down the fitted model
--

*Let* 

`$$u=exp(-2.91+0.11RPW)$$` 
*then*

$$ \hat{won}=\frac{u}{1+u} $$



---
# Model fit


```
#&gt; 
#&gt; Call:
#&gt; glm(formula = won ~ Receiving.Points.Won, family = binomial(link = "logit"), 
#&gt;     data = tennis)
#&gt; 
#&gt; Deviance Residuals: 
#&gt;    Min      1Q  Median      3Q     Max  
#&gt; -2.506   0.227   0.411   0.624   1.877  
#&gt; 
#&gt; Coefficients:
#&gt;                      Estimate Std. Error z value Pr(&gt;|z|)    
#&gt; (Intercept)           -2.9053     0.5860   -4.96  7.1e-07 ***
#&gt; Receiving.Points.Won   0.1111     0.0152    7.29  3.0e-13 ***
#&gt; ---
#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
#&gt; 
#&gt; (Dispersion parameter for binomial family taken to be 1)
#&gt; 
#&gt;     Null deviance: 472.99  on 511  degrees of freedom
#&gt; Residual deviance: 402.16  on 510  degrees of freedom
#&gt; AIC: 406.2
#&gt; 
#&gt; Number of Fisher Scoring iterations: 5
```

Not much difference between null and residual deviance, suggests return points won does not explain much of the match result. 

---
# Residuals

![](week6.class2_files/figure-html/unnamed-chunk-14-1.png)&lt;!-- --&gt;

Model is just not capturing the data very well. There are two groups of residuals, its overfitting a chunk and underfitting chunks of data. 

---
# Influence statistics


```
#&gt;     .cooksd .resid
#&gt; 1   6.0e-02  1.877
#&gt; 2   3.6e-02 -2.505
#&gt; 3   2.9e-02 -2.420
#&gt; 4   2.4e-02  1.528
#&gt; 5   2.0e-02 -2.287
#&gt; 6   1.7e-02 -2.242
#&gt; 7   1.7e-02 -2.242
#&gt; 8   1.5e-02 -2.196
#&gt; 9   1.3e-02 -2.149
#&gt; 10  1.2e-02  1.329
#&gt; 11  1.2e-02  1.329
#&gt; 12  1.1e-02 -2.103
#&gt; 13  1.1e-02 -2.103
#&gt; 14  1.1e-02 -2.103
#&gt; 15  9.9e-03 -2.055
#&gt; 16  9.9e-03 -2.055
#&gt; 17  9.9e-03 -2.055
#&gt; 18  9.9e-03 -2.055
#&gt; 19  9.4e-03  1.280
#&gt; 20  9.4e-03  1.280
#&gt; 21  9.4e-03  1.280
#&gt; 22  9.4e-03  1.280
#&gt; 23  8.6e-03 -2.008
#&gt; 24  7.6e-03  1.232
#&gt; 25  7.6e-03  1.232
#&gt; 26  7.5e-03 -1.959
#&gt; 27  7.5e-03 -1.959
#&gt; 28  7.5e-03 -1.959
#&gt; 29  7.5e-03 -1.959
#&gt; 30  7.5e-03 -1.959
#&gt; 31  7.5e-03 -1.959
#&gt; 32  6.6e-03 -1.911
#&gt; 33  6.6e-03 -1.911
#&gt; 34  5.9e-03 -1.124
#&gt; 35  5.9e-03 -1.170
#&gt; 36  5.9e-03 -1.862
#&gt; 37  5.9e-03 -1.862
#&gt; 38  5.9e-03 -1.078
#&gt; 39  5.9e-03 -1.078
#&gt; 40  5.7e-03 -1.266
#&gt; 41  5.7e-03 -1.266
#&gt; 42  5.6e-03 -1.315
#&gt; 43  5.6e-03 -1.315
#&gt; 44  5.6e-03 -1.315
#&gt; 45  5.6e-03 -1.315
#&gt; 46  5.6e-03 -1.315
#&gt; 47  5.6e-03 -1.315
#&gt; 48  5.6e-03 -0.989
#&gt; 49  5.4e-03 -1.364
#&gt; 50  5.4e-03 -1.364
#&gt; 51  5.4e-03 -1.364
#&gt; 52  5.4e-03 -1.364
#&gt; 53  5.4e-03 -0.946
#&gt; 54  5.3e-03 -1.813
#&gt; 55  5.3e-03 -1.813
#&gt; 56  5.3e-03 -1.813
#&gt; 57  5.2e-03 -1.413
#&gt; 58  5.2e-03 -1.413
#&gt; 59  5.2e-03 -1.413
#&gt; 60  5.2e-03 -1.413
#&gt; 61  5.2e-03 -1.413
#&gt; 62  5.2e-03 -1.413
#&gt; 63  5.2e-03 -1.413
#&gt; 64  5.0e-03 -1.463
#&gt; 65  5.0e-03 -1.463
#&gt; 66  5.0e-03 -1.463
#&gt; 67  5.0e-03 -1.463
#&gt; 68  5.0e-03 -1.463
#&gt; 69  5.0e-03 -1.463
#&gt; 70  5.0e-03 -1.763
#&gt; 71  5.0e-03 -1.763
#&gt; 72  5.0e-03 -1.763
#&gt; 73  5.0e-03 -1.763
#&gt; 74  5.0e-03 -1.763
#&gt; 75  4.9e-03 -1.513
#&gt; 76  4.9e-03 -1.513
#&gt; 77  4.9e-03 -1.513
#&gt; 78  4.9e-03 -1.513
#&gt; 79  4.9e-03 -1.513
#&gt; 80  4.9e-03 -1.513
#&gt; 81  4.9e-03 -1.513
#&gt; 82  4.8e-03  1.138
#&gt; 83  4.8e-03  1.138
#&gt; 84  4.8e-03  1.138
#&gt; 85  4.8e-03  1.138
#&gt; 86  4.8e-03  1.138
#&gt; 87  4.8e-03 -1.713
#&gt; 88  4.8e-03 -1.713
#&gt; 89  4.8e-03 -1.713
#&gt; 90  4.8e-03 -1.713
#&gt; 91  4.7e-03 -1.563
#&gt; 92  4.7e-03 -1.563
#&gt; 93  4.7e-03 -1.563
#&gt; 94  4.6e-03 -1.663
#&gt; 95  4.6e-03 -1.663
#&gt; 96  4.6e-03 -1.663
#&gt; 97  4.6e-03 -1.663
#&gt; 98  4.6e-03 -1.613
#&gt; 99  4.6e-03 -1.613
#&gt; 100 4.6e-03 -1.613
#&gt; 101 4.6e-03 -1.613
#&gt; 102 4.6e-03 -1.613
#&gt; 103 4.6e-03 -1.613
#&gt; 104 3.8e-03  1.091
#&gt; 105 3.8e-03  1.091
#&gt; 106 3.8e-03  1.091
#&gt; 107 3.0e-03  1.046
#&gt; 108 3.0e-03  1.046
#&gt; 109 3.0e-03  1.046
#&gt; 110 2.6e-03 -0.614
#&gt; 111 2.3e-03  1.002
#&gt; 112 2.3e-03  1.002
#&gt; 113 2.3e-03  1.002
#&gt; 114 2.3e-03  1.002
#&gt; 115 2.3e-03  1.002
#&gt; 116 2.3e-03  1.002
#&gt; 117 2.3e-03  1.002
#&gt; 118 1.8e-03  0.959
#&gt; 119 1.8e-03  0.959
#&gt; 120 1.8e-03  0.959
#&gt; 121 1.8e-03  0.959
#&gt; 122 1.8e-03  0.959
#&gt; 123 1.8e-03  0.959
#&gt; 124 1.8e-03  0.959
#&gt; 125 1.8e-03  0.959
#&gt; 126 1.8e-03  0.959
#&gt; 127 1.8e-03  0.959
#&gt; 128 1.8e-03  0.959
#&gt; 129 1.4e-03  0.917
#&gt; 130 1.4e-03  0.917
#&gt; 131 1.4e-03  0.917
#&gt; 132 1.4e-03  0.917
#&gt; 133 1.4e-03  0.917
#&gt; 134 1.4e-03  0.917
#&gt; 135 1.4e-03  0.917
#&gt; 136 1.4e-03  0.917
#&gt; 137 1.4e-03  0.917
#&gt; 138 1.4e-03  0.917
#&gt; 139 1.4e-03  0.917
#&gt; 140 1.1e-03  0.876
#&gt; 141 1.1e-03  0.876
#&gt; 142 1.1e-03  0.876
#&gt; 143 1.1e-03  0.876
#&gt; 144 1.1e-03  0.876
#&gt; 145 1.1e-03  0.876
#&gt; 146 1.1e-03  0.876
#&gt; 147 8.3e-04  0.836
#&gt; 148 8.3e-04  0.836
#&gt; 149 8.3e-04  0.836
#&gt; 150 8.3e-04  0.836
#&gt; 151 8.3e-04  0.836
#&gt; 152 8.3e-04  0.836
#&gt; 153 8.3e-04  0.836
#&gt; 154 6.5e-04  0.797
#&gt; 155 6.5e-04  0.797
#&gt; 156 6.5e-04  0.797
#&gt; 157 6.5e-04  0.797
#&gt; 158 6.5e-04  0.797
#&gt; 159 6.5e-04  0.797
#&gt; 160 6.5e-04  0.797
#&gt; 161 6.5e-04  0.797
#&gt; 162 6.5e-04  0.797
#&gt; 163 6.5e-04  0.797
#&gt; 164 6.5e-04  0.797
#&gt; 165 6.5e-04  0.797
#&gt; 166 6.5e-04  0.797
#&gt; 167 6.5e-04  0.797
#&gt; 168 6.5e-04  0.797
#&gt; 169 5.2e-04  0.760
#&gt; 170 5.2e-04  0.760
#&gt; 171 5.2e-04  0.760
#&gt; 172 5.2e-04  0.760
#&gt; 173 5.2e-04  0.760
#&gt; 174 5.2e-04  0.760
#&gt; 175 5.2e-04  0.760
#&gt; 176 5.2e-04  0.760
#&gt; 177 5.2e-04  0.760
#&gt; 178 5.2e-04  0.760
#&gt; 179 5.2e-04  0.760
#&gt; 180 5.2e-04  0.760
#&gt; 181 4.3e-04  0.724
#&gt; 182 4.3e-04  0.724
#&gt; 183 4.3e-04  0.724
#&gt; 184 4.3e-04  0.724
#&gt; 185 4.3e-04  0.724
#&gt; 186 4.3e-04  0.724
#&gt; 187 4.3e-04  0.724
#&gt; 188 4.3e-04  0.724
#&gt; 189 4.3e-04  0.724
#&gt; 190 4.3e-04  0.724
#&gt; 191 4.3e-04  0.724
#&gt; 192 4.3e-04  0.724
#&gt; 193 4.3e-04  0.724
#&gt; 194 4.3e-04  0.724
#&gt; 195 3.6e-04  0.689
#&gt; 196 3.6e-04  0.689
#&gt; 197 3.6e-04  0.689
#&gt; 198 3.6e-04  0.689
#&gt; 199 3.6e-04  0.689
#&gt; 200 3.6e-04  0.689
#&gt; 201 3.6e-04  0.689
#&gt; 202 3.6e-04  0.689
#&gt; 203 3.6e-04  0.689
#&gt; 204 3.6e-04  0.689
#&gt; 205 3.6e-04  0.689
#&gt; 206 3.1e-04  0.656
#&gt; 207 3.1e-04  0.656
#&gt; 208 3.1e-04  0.656
#&gt; 209 3.1e-04  0.656
#&gt; 210 3.1e-04  0.656
#&gt; 211 3.1e-04  0.656
#&gt; 212 3.1e-04  0.656
#&gt; 213 3.1e-04  0.656
#&gt; 214 2.7e-04  0.624
#&gt; 215 2.7e-04  0.624
#&gt; 216 2.7e-04  0.624
#&gt; 217 2.7e-04  0.624
#&gt; 218 2.7e-04  0.624
#&gt; 219 2.7e-04  0.624
#&gt; 220 2.7e-04  0.624
#&gt; 221 2.4e-04  0.593
#&gt; 222 2.4e-04  0.593
#&gt; 223 2.4e-04  0.593
#&gt; 224 2.4e-04  0.593
#&gt; 225 2.4e-04  0.593
#&gt; 226 2.4e-04  0.593
#&gt; 227 2.4e-04  0.593
#&gt; 228 2.4e-04  0.593
#&gt; 229 2.4e-04  0.593
#&gt; 230 2.4e-04  0.593
#&gt; 231 2.4e-04  0.593
#&gt; 232 2.4e-04  0.593
#&gt; 233 2.4e-04  0.593
#&gt; 234 2.4e-04  0.593
#&gt; 235 2.4e-04  0.593
#&gt; 236 2.2e-04  0.563
#&gt; 237 2.2e-04  0.563
#&gt; 238 2.2e-04  0.563
#&gt; 239 2.2e-04  0.563
#&gt; 240 2.2e-04  0.563
#&gt; 241 2.2e-04  0.563
#&gt; 242 2.2e-04  0.563
#&gt; 243 2.2e-04  0.563
#&gt; 244 2.2e-04  0.563
#&gt; 245 2.2e-04  0.563
#&gt; 246 2.2e-04  0.563
#&gt; 247 2.2e-04  0.563
#&gt; 248 2.2e-04  0.563
#&gt; 249 2.2e-04  0.563
#&gt; 250 2.2e-04  0.563
#&gt; 251 2.2e-04  0.563
#&gt; 252 2.2e-04  0.563
#&gt; 253 2.2e-04  0.563
#&gt; 254 2.2e-04  0.563
#&gt; 255 2.0e-04  0.535
#&gt; 256 2.0e-04  0.535
#&gt; 257 2.0e-04  0.535
#&gt; 258 2.0e-04  0.535
#&gt; 259 2.0e-04  0.535
#&gt; 260 2.0e-04  0.535
#&gt; 261 2.0e-04  0.535
#&gt; 262 2.0e-04  0.535
#&gt; 263 2.0e-04  0.535
#&gt; 264 2.0e-04  0.535
#&gt; 265 2.0e-04  0.535
#&gt; 266 2.0e-04  0.535
#&gt; 267 2.0e-04  0.535
#&gt; 268 2.0e-04  0.535
#&gt; 269 2.0e-04  0.535
#&gt; 270 2.0e-04  0.535
#&gt; 271 2.0e-04  0.535
#&gt; 272 1.9e-04  0.508
#&gt; 273 1.9e-04  0.508
#&gt; 274 1.9e-04  0.508
#&gt; 275 1.9e-04  0.508
#&gt; 276 1.9e-04  0.508
#&gt; 277 1.9e-04  0.508
#&gt; 278 1.9e-04  0.508
#&gt; 279 1.9e-04  0.508
#&gt; 280 1.9e-04  0.508
#&gt; 281 1.9e-04  0.508
#&gt; 282 1.9e-04  0.508
#&gt; 283 1.9e-04  0.508
#&gt; 284 1.9e-04  0.508
#&gt; 285 1.9e-04  0.508
#&gt; 286 1.9e-04  0.508
#&gt; 287 1.9e-04  0.508
#&gt; 288 1.9e-04  0.508
#&gt; 289 1.9e-04  0.508
#&gt; 290 1.7e-04  0.482
#&gt; 291 1.7e-04  0.482
#&gt; 292 1.7e-04  0.482
#&gt; 293 1.7e-04  0.482
#&gt; 294 1.7e-04  0.482
#&gt; 295 1.7e-04  0.482
#&gt; 296 1.7e-04  0.482
#&gt; 297 1.7e-04  0.482
#&gt; 298 1.7e-04  0.482
#&gt; 299 1.7e-04  0.482
#&gt; 300 1.7e-04  0.482
#&gt; 301 1.7e-04  0.482
#&gt; 302 1.7e-04  0.482
#&gt; 303 1.7e-04  0.482
#&gt; 304 1.7e-04  0.482
#&gt; 305 1.7e-04  0.482
#&gt; 306 1.7e-04  0.482
#&gt; 307 1.7e-04  0.482
#&gt; 308 1.7e-04  0.482
#&gt; 309 1.7e-04  0.482
#&gt; 310 1.7e-04  0.482
#&gt; 311 1.7e-04  0.482
#&gt; 312 1.7e-04  0.482
#&gt; 313 1.6e-04  0.457
#&gt; 314 1.6e-04  0.457
#&gt; 315 1.6e-04  0.457
#&gt; 316 1.6e-04  0.457
#&gt; 317 1.6e-04  0.457
#&gt; 318 1.6e-04  0.457
#&gt; 319 1.6e-04  0.457
#&gt; 320 1.6e-04  0.457
#&gt; 321 1.6e-04  0.457
#&gt; 322 1.6e-04  0.457
#&gt; 323 1.6e-04  0.457
#&gt; 324 1.6e-04  0.457
#&gt; 325 1.6e-04  0.457
#&gt; 326 1.5e-04  0.434
#&gt; 327 1.5e-04  0.434
#&gt; 328 1.5e-04  0.434
#&gt; 329 1.5e-04  0.434
#&gt; 330 1.5e-04  0.434
#&gt; 331 1.5e-04  0.434
#&gt; 332 1.5e-04  0.434
#&gt; 333 1.5e-04  0.434
#&gt; 334 1.5e-04  0.434
#&gt; 335 1.5e-04  0.434
#&gt; 336 1.5e-04  0.434
#&gt; 337 1.5e-04  0.434
#&gt; 338 1.5e-04  0.434
#&gt; 339 1.5e-04  0.434
#&gt; 340 1.5e-04  0.434
#&gt; 341 1.5e-04  0.434
#&gt; 342 1.5e-04  0.434
#&gt; 343 1.4e-04  0.411
#&gt; 344 1.4e-04  0.411
#&gt; 345 1.4e-04  0.411
#&gt; 346 1.4e-04  0.411
#&gt; 347 1.4e-04  0.411
#&gt; 348 1.4e-04  0.411
#&gt; 349 1.4e-04  0.411
#&gt; 350 1.4e-04  0.411
#&gt; 351 1.4e-04  0.411
#&gt; 352 1.4e-04  0.411
#&gt; 353 1.4e-04  0.411
#&gt; 354 1.2e-04  0.390
#&gt; 355 1.2e-04  0.390
#&gt; 356 1.2e-04  0.390
#&gt; 357 1.2e-04  0.390
#&gt; 358 1.2e-04  0.390
#&gt; 359 1.2e-04  0.390
#&gt; 360 1.2e-04  0.390
#&gt; 361 1.2e-04  0.390
#&gt; 362 1.2e-04  0.390
#&gt; 363 1.2e-04  0.390
#&gt; 364 1.2e-04  0.390
#&gt; 365 1.2e-04  0.390
#&gt; 366 1.2e-04  0.390
#&gt; 367 1.2e-04  0.390
#&gt; 368 1.2e-04  0.390
#&gt; 369 1.1e-04  0.370
#&gt; 370 1.1e-04  0.370
#&gt; 371 1.1e-04  0.370
#&gt; 372 1.1e-04  0.370
#&gt; 373 1.1e-04  0.370
#&gt; 374 1.1e-04  0.370
#&gt; 375 1.1e-04  0.370
#&gt; 376 1.1e-04  0.370
#&gt; 377 1.1e-04  0.370
#&gt; 378 1.1e-04  0.370
#&gt; 379 1.1e-04  0.370
#&gt; 380 1.1e-04  0.370
#&gt; 381 1.1e-04  0.370
#&gt; 382 1.1e-04  0.370
#&gt; 383 1.1e-04  0.370
#&gt; 384 1.0e-04  0.350
#&gt; 385 1.0e-04  0.350
#&gt; 386 1.0e-04  0.350
#&gt; 387 1.0e-04  0.350
#&gt; 388 1.0e-04  0.350
#&gt; 389 1.0e-04  0.350
#&gt; 390 1.0e-04  0.350
#&gt; 391 1.0e-04  0.350
#&gt; 392 1.0e-04  0.350
#&gt; 393 9.3e-05  0.332
#&gt; 394 9.3e-05  0.332
#&gt; 395 9.3e-05  0.332
#&gt; 396 9.3e-05  0.332
#&gt; 397 9.3e-05  0.332
#&gt; 398 9.3e-05  0.332
#&gt; 399 9.3e-05  0.332
#&gt; 400 9.3e-05  0.332
#&gt; 401 9.3e-05  0.332
#&gt; 402 9.3e-05  0.332
#&gt; 403 9.3e-05  0.332
#&gt; 404 9.3e-05  0.332
#&gt; 405 9.3e-05  0.332
#&gt; 406 8.3e-05  0.314
#&gt; 407 8.3e-05  0.314
#&gt; 408 8.3e-05  0.314
#&gt; 409 8.3e-05  0.314
#&gt; 410 8.3e-05  0.314
#&gt; 411 8.3e-05  0.314
#&gt; 412 8.3e-05  0.314
#&gt; 413 8.3e-05  0.314
#&gt; 414 8.3e-05  0.314
#&gt; 415 8.3e-05  0.314
#&gt; 416 8.3e-05  0.314
#&gt; 417 8.3e-05  0.314
#&gt; 418 7.4e-05  0.298
#&gt; 419 7.4e-05  0.298
#&gt; 420 7.4e-05  0.298
#&gt; 421 7.4e-05  0.298
#&gt; 422 7.4e-05  0.298
#&gt; 423 7.4e-05  0.298
#&gt; 424 7.4e-05  0.298
#&gt; 425 7.4e-05  0.298
#&gt; 426 7.4e-05  0.298
#&gt; 427 7.4e-05  0.298
#&gt; 428 7.4e-05  0.298
#&gt; 429 7.4e-05  0.298
#&gt; 430 7.4e-05  0.298
#&gt; 431 7.4e-05  0.298
#&gt; 432 7.4e-05  0.298
#&gt; 433 7.4e-05  0.298
#&gt; 434 6.6e-05  0.282
#&gt; 435 6.6e-05  0.282
#&gt; 436 6.6e-05  0.282
#&gt; 437 6.6e-05  0.282
#&gt; 438 6.6e-05  0.282
#&gt; 439 6.6e-05  0.282
#&gt; 440 6.6e-05  0.282
#&gt; 441 6.6e-05  0.282
#&gt; 442 6.6e-05  0.282
#&gt; 443 6.6e-05  0.282
#&gt; 444 6.6e-05  0.282
#&gt; 445 6.6e-05  0.282
#&gt; 446 6.6e-05  0.282
#&gt; 447 6.6e-05  0.282
#&gt; 448 5.8e-05  0.267
#&gt; 449 5.8e-05  0.267
#&gt; 450 5.8e-05  0.267
#&gt; 451 5.8e-05  0.267
#&gt; 452 5.8e-05  0.267
#&gt; 453 5.8e-05  0.267
#&gt; 454 5.8e-05  0.267
#&gt; 455 5.8e-05  0.267
#&gt; 456 5.8e-05  0.267
#&gt; 457 5.8e-05  0.267
#&gt; 458 5.1e-05  0.253
#&gt; 459 5.1e-05  0.253
#&gt; 460 5.1e-05  0.253
#&gt; 461 5.1e-05  0.253
#&gt; 462 5.1e-05  0.253
#&gt; 463 5.1e-05  0.253
#&gt; 464 5.1e-05  0.253
#&gt; 465 5.1e-05  0.253
#&gt; 466 5.1e-05  0.253
#&gt; 467 4.5e-05  0.239
#&gt; 468 4.5e-05  0.239
#&gt; 469 4.5e-05  0.239
#&gt; 470 4.5e-05  0.239
#&gt; 471 4.5e-05  0.239
#&gt; 472 4.0e-05  0.227
#&gt; 473 4.0e-05  0.227
#&gt; 474 4.0e-05  0.227
#&gt; 475 4.0e-05  0.227
#&gt; 476 4.0e-05  0.227
#&gt; 477 4.0e-05  0.227
#&gt; 478 4.0e-05  0.227
#&gt; 479 4.0e-05  0.227
#&gt; 480 4.0e-05  0.227
#&gt; 481 3.4e-05  0.214
#&gt; 482 3.4e-05  0.214
#&gt; 483 3.4e-05  0.214
#&gt; 484 3.4e-05  0.214
#&gt; 485 3.4e-05  0.214
#&gt; 486 3.4e-05  0.214
#&gt; 487 3.4e-05  0.214
#&gt; 488 3.0e-05  0.203
#&gt; 489 3.0e-05  0.203
#&gt; 490 2.6e-05  0.192
#&gt; 491 2.6e-05  0.192
#&gt; 492 2.6e-05  0.192
#&gt; 493 2.6e-05  0.192
#&gt; 494 2.2e-05  0.182
#&gt; 495 2.2e-05  0.182
#&gt; 496 2.2e-05  0.182
#&gt; 497 2.2e-05  0.182
#&gt; 498 2.2e-05  0.182
#&gt; 499 1.9e-05  0.172
#&gt; 500 1.9e-05  0.172
#&gt; 501 1.9e-05  0.172
#&gt; 502 1.9e-05  0.172
#&gt; 503 1.7e-05  0.163
#&gt; 504 1.7e-05  0.163
#&gt; 505 1.0e-05  0.138
#&gt; 506 7.5e-06  0.124
#&gt; 507 5.4e-06  0.111
#&gt; 508 3.9e-06  0.099
#&gt; 509 3.3e-06  0.094
#&gt; 510 2.8e-06  0.089
#&gt; 511 2.8e-06  0.089
#&gt; 512 2.0e-06  0.079
```

No influential observations.

---
# Prediction from the model


```r
newdata &lt;- data.frame(Receiving.Points.Won=c(20, 50), won=c(NA, NA))
predict(tennis_glm, newdata, type="response")
#&gt;    1    2 
#&gt; 0.34 0.93
```

Interpret the response as the probability of winning if your receiving points was 20, 50. 

---
# Summary

Generalised linear models are a systematic way to fit different types of response distributions. 

---
# Resources

- [Beginners guide](https://www.analyticsvidhya.com/blog/2015/11/beginners-guide-on-logistic-regression-in-r/)
- [Introduction to GLMs](https://onlinecourses.science.psu.edu/stat504/node/216)
- [Quick-R GLMs](http://www.statmethods.net/advstats/glm.html)
- [The Analysis Factor, Generalized Linear Models Parts 1-4](http://www.theanalysisfactor.com/resources/by-topic/r/)
- [wikipedia](https://en.wikipedia.org/wiki/Generalized_linear_model)
- [Do Smashes Win Matches?](http://onlinelibrary.wiley.com/doi/10.1111/j.1740-9713.2013.00665.x/full)

---
class: inverse middle 
# Share and share alike

&lt;a rel="license" href="http://creativecommons.org/licenses/by/4.0/"&gt;&lt;img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by/4.0/88x31.png" /&gt;&lt;/a&gt;&lt;br /&gt;This work is licensed under a &lt;a rel="license" href="http://creativecommons.org/licenses/by/4.0/"&gt;Creative Commons Attribution 4.0 International License&lt;/a&gt;.
    </textarea>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {window.dispatchEvent(new Event('resize'));});</script>

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre']
  }
});
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
